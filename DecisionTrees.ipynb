{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Decision Trees\n",
    "# \n",
    "# *Adapted from Chapter 8 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)*\n",
    "\n",
    "# Why are we learning about decision trees?\n",
    "# \n",
    "# - Can be applied to both regression and classification problems\n",
    "# - Many useful properties\n",
    "# - Very popular\n",
    "# - Basis for more sophisticated models\n",
    "# - Have a different way of \"thinking\" than the other models we have studied\n",
    "\n",
    "# ## Lesson objectives\n",
    "# \n",
    "# Students will be able to:\n",
    "# \n",
    "# - Explain how a decision tree is created\n",
    "# - Build a decision tree model in scikit-learn\n",
    "# - Tune a decision tree model and explain how tuning impacts the model\n",
    "# - Interpret a tree diagram\n",
    "# - Describe the key differences between regression and classification trees\n",
    "# - Decide whether a decision tree is an appropriate model for a given problem\n",
    "\n",
    "# # Part 1: Regression trees\n",
    "# \n",
    "# Major League Baseball player data from 1986-87:\n",
    "# \n",
    "# - **Years** (x-axis): number of years playing in the major leagues\n",
    "# - **Hits** (y-axis): number of hits in the previous year\n",
    "# - **Salary** (color): low salary is blue/green, high salary is red/yellow\n",
    "\n",
    "# ![Salary data](images/salary_color.png)\n",
    "\n",
    "# Group exercise:\n",
    "# \n",
    "# - The data above is our **training data**.\n",
    "# - We want to build a model that predicts the Salary of **future players** based on Years and Hits.\n",
    "# - We are going to \"segment\" the feature space into regions, and then use the **mean Salary in each region** as the predicted Salary for future players.\n",
    "# - Intuitively, you want to **maximize** the similarity (or \"homogeneity\") within a given region, and **minimize** the similarity between different regions.\n",
    "# \n",
    "# Rules for segmenting:\n",
    "# \n",
    "# - You can only use **straight lines**, drawn one at a time.\n",
    "# - Your line must either be **vertical or horizontal**.\n",
    "# - Your line **stops** when it hits an existing line.\n",
    "\n",
    "# ![Salary regions](images/salary_regions.png)\n",
    "\n",
    "# Above are the regions created by a computer:\n",
    "# \n",
    "# - $R_1$: players with **less than 5 years** of experience, mean Salary of **\\$166,000 **\n",
    "# - $R_2$: players with **5 or more years** of experience and **less than 118 hits**, mean Salary of **\\$403,000 **\n",
    "# - $R_3$: players with **5 or more years** of experience and **118 hits or more**, mean Salary of **\\$846,000 **\n",
    "# \n",
    "# **Note:** Years and Hits are both integers, but the convention is to use the **midpoint** between adjacent values to label a split.\n",
    "# \n",
    "# These regions are used to make predictions on **out-of-sample data**. Thus, there are only three possible predictions! (Is this different from how **linear regression** makes predictions?)\n",
    "# \n",
    "# Below is the equivalent regression tree:\n",
    "\n",
    "# ![Salary tree](images/salary_tree.png)\n",
    "\n",
    "# The first split is **Years < 4.5**, thus that split goes at the top of the tree. When a splitting rule is **True**, you follow the left branch. When a splitting rule is **False**, you follow the right branch.\n",
    "# \n",
    "# For players in the **left branch**, the mean Salary is \\$166,000, thus you label it with that value. (Salary has been divided by 1000 and log-transformed to 5.11.)\n",
    "# \n",
    "# For players in the **right branch**, there is a further split on **Hits < 117.5**, dividing players into two more Salary regions: \\$403,000 (transformed to 6.00), and \\$846,000 (transformed to 6.74).\n",
    "\n",
    "# ![Salary tree annotated](images/salary_tree_annotated.png)\n",
    "\n",
    "# **What does this tree tell you about your data?**\n",
    "# \n",
    "# - Years is the most important factor determining Salary, with a lower number of Years corresponding to a lower Salary.\n",
    "# - For a player with a lower number of Years, Hits is not an important factor determining Salary.\n",
    "# - For a player with a higher number of Years, Hits is an important factor determining Salary, with a greater number of Hits corresponding to a higher Salary.\n",
    "# \n",
    "# **Question:** What do you like and dislike about decision trees so far?\n",
    "\n",
    "# ## Building a regression tree by hand\n",
    "# \n",
    "# Your **training data** is a tiny dataset of [used vehicle sale prices](https://raw.githubusercontent.com/justmarkham/DAT8/master/data/vehicles_train.csv). Your goal is to **predict price** for testing data.\n",
    "# \n",
    "# 1. Read the data into a Pandas DataFrame.\n",
    "# 2. Explore the data by sorting, plotting, or split-apply-combine (aka `group_by`).\n",
    "# 3. Decide which feature is the most important predictor, and use that to create your first splitting rule.\n",
    "#     - Only binary splits are allowed.\n",
    "# 4. After making your first split, split your DataFrame into two parts, and then explore each part to figure out what other splits to make.\n",
    "# 5. Stop making splits once you are convinced that it strikes a good balance between underfitting and overfitting.\n",
    "#     - Your goal is to build a model that generalizes well.\n",
    "#     - You are allowed to split on the same variable multiple times!\n",
    "# 6. Draw your tree, labeling the leaves with the mean price for the observations in that region.\n",
    "#     - Make sure nothing is backwards: You follow the **left branch** if the rule is true, and the **right branch** if the rule is false.\n",
    "\n",
    "# ## How does a computer build a regression tree?\n",
    "# \n",
    "# **Ideal approach:** Consider every possible partition of the feature space (computationally infeasible)\n",
    "# \n",
    "# **\"Good enough\" approach:** recursive binary splitting\n",
    "# \n",
    "# 1. Begin at the top of the tree.\n",
    "# 2. For **every feature**, examine **every possible cutpoint**, and choose the feature and cutpoint such that the resulting tree has the lowest possible mean squared error (MSE). Make that split.\n",
    "# 3. Examine the two resulting regions, and again make a **single split** (in one of the regions) to minimize the MSE.\n",
    "# 4. Keep repeating step 3 until a **stopping criterion** is met:\n",
    "#     - maximum tree depth (maximum number of splits required to arrive at a leaf)\n",
    "#     - minimum number of observations in a leaf\n",
    "\n",
    "# ### Demo: Choosing the ideal cutpoint for a given feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vehicle data\n",
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/vehicles_train.csv'\n",
    "train = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>vtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22000</td>\n",
       "      <td>2012</td>\n",
       "      <td>13000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13000</td>\n",
       "      <td>2010</td>\n",
       "      <td>73500</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9500</td>\n",
       "      <td>2009</td>\n",
       "      <td>78000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9000</td>\n",
       "      <td>2007</td>\n",
       "      <td>47000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  year  miles  doors vtype\n",
       "0  22000  2012  13000      2   car\n",
       "1  14000  2010  30000      2   car\n",
       "2  13000  2010  73500      4   car\n",
       "3   9500  2009  78000      4   car\n",
       "4   9000  2007  47000      4   car"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>vtype</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22000</td>\n",
       "      <td>2012</td>\n",
       "      <td>13000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13000</td>\n",
       "      <td>2010</td>\n",
       "      <td>73500</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9500</td>\n",
       "      <td>2009</td>\n",
       "      <td>78000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9000</td>\n",
       "      <td>2007</td>\n",
       "      <td>47000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4000</td>\n",
       "      <td>2006</td>\n",
       "      <td>124000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3000</td>\n",
       "      <td>2004</td>\n",
       "      <td>177000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000</td>\n",
       "      <td>2004</td>\n",
       "      <td>209000</td>\n",
       "      <td>4</td>\n",
       "      <td>truck</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>138000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1900</td>\n",
       "      <td>2003</td>\n",
       "      <td>160000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2500</td>\n",
       "      <td>2003</td>\n",
       "      <td>190000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5000</td>\n",
       "      <td>2001</td>\n",
       "      <td>62000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1800</td>\n",
       "      <td>1999</td>\n",
       "      <td>163000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1300</td>\n",
       "      <td>1997</td>\n",
       "      <td>138000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  year   miles  doors  vtype   prediction\n",
       "0   22000  2012   13000      2    car  6571.428571\n",
       "1   14000  2010   30000      2    car  6571.428571\n",
       "2   13000  2010   73500      4    car  6571.428571\n",
       "3    9500  2009   78000      4    car  6571.428571\n",
       "4    9000  2007   47000      4    car  6571.428571\n",
       "5    4000  2006  124000      2    car  6571.428571\n",
       "6    3000  2004  177000      4    car  6571.428571\n",
       "7    2000  2004  209000      4  truck  6571.428571\n",
       "8    3000  2003  138000      2    car  6571.428571\n",
       "9    1900  2003  160000      4    car  6571.428571\n",
       "10   2500  2003  190000      2  truck  6571.428571\n",
       "11   5000  2001   62000      4    car  6571.428571\n",
       "12   1800  1999  163000      2  truck  6571.428571\n",
       "13   1300  1997  138000      4    car  6571.428571"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before splitting anything, just predict the mean of the entire dataset\n",
    "train['prediction'] = train.price.mean()\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5936.981985995983"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate RMSE for those predictions\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "np.sqrt(metrics.mean_squared_error(train.price, train.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that calculates the RMSE for a given split of miles\n",
    "def mileage_split(miles):\n",
    "    lower_mileage_price = train[train.miles < miles].price.mean()\n",
    "    higher_mileage_price = train[train.miles >= miles].price.mean()\n",
    "    train['prediction'] = np.where(train.miles < miles, lower_mileage_price, higher_mileage_price)\n",
    "    return np.sqrt(metrics.mean_squared_error(train.price, train.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3984.0917425414564\n"
     ]
    }
   ],
   "source": [
    "# calculate RMSE for tree which splits on miles < 50000\n",
    "print ('RMSE:', mileage_split(50000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3530.146530076269\n"
     ]
    }
   ],
   "source": [
    "# calculate RMSE for tree which splits on miles < 100000\n",
    "print ('RMSE:', mileage_split(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all possible mileage splits\n",
    "mileage_range = range(train.miles.min(), train.miles.max(), 1000)\n",
    "RMSE = [mileage_split(miles) for miles in mileage_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow plots to appear in the notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'RMSE (lower is better)')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot mileage cutpoint (x-axis) versus RMSE (y-axis)\n",
    "plt.plot(mileage_range, RMSE)\n",
    "plt.xlabel('Mileage cutpoint')\n",
    "plt.ylabel('RMSE (lower is better)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Recap:** Before every split, this process is repeated for every feature, and the feature and cutpoint that produces the lowest MSE is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Building a regression tree in scikit-learn\n",
    "# encode car as 0 and truck as 1\n",
    "train['vtype'] = train.vtype.map({'car':0, 'truck':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y\n",
    "feature_cols = ['year', 'miles', 'doors', 'vtype']\n",
    "X = train[feature_cols]\n",
    "y = train.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate a DecisionTreeRegressor (with random_state=1)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "treereg = DecisionTreeRegressor(random_state=1)\n",
    "treereg.fit(X,y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use leave-one-out cross-validation (LOOCV) to estimate the RMSE for this model\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics as mt\n",
    "predict=treereg.predict(X)\n",
    "mt.mean_squared_error(y,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(treereg, X, y, cv=14)\n",
    "np.mean(np.sqrt(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## What happens when we grow a tree too deep?\n",
    "# \n",
    "# - Left: Regression tree for Salary **grown deeper**\n",
    "# - Right: Comparison of the **training, testing, and cross-validation errors** for trees with different numbers of leaves\n",
    "\n",
    "# ![Salary tree grown deep](images/salary_tree_deep.png)\n",
    "\n",
    "# The **training error** continues to go down as the tree size increases (due to overfitting), but the lowest **cross-validation error** occurs for a tree with 3 leaves.\n",
    "\n",
    "# ## Tuning a regression tree\n",
    "# \n",
    "# Let's try to reduce the RMSE by tuning the **max_depth** parameter:\n",
    "\n",
    "# try different values one-by-one\n",
    "treereg = DecisionTreeRegressor(max_depth=1, random_state=1)\n",
    "scores = cross_val_score(treereg, X, y, cv=14)\n",
    "np.mean(np.sqrt(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in sqrt\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'RMSE (lower is better)')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Or, we could write a loop to try a range of values:\n",
    "\n",
    "# list of values to try\n",
    "max_depth_range = range(1, 8)\n",
    "\n",
    "# list to store the average RMSE for each value of max_depth\n",
    "RMSE_scores = []\n",
    "\n",
    "# use LOOCV with each value of max_depth\n",
    "for depth in max_depth_range:\n",
    "    treereg = DecisionTreeRegressor(max_depth=depth, random_state=1)\n",
    "    MSE_scores = cross_val_score(treereg, X, y, cv=14)\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))\n",
    "\n",
    "\n",
    "# plot max_depth (x-axis) versus RMSE (y-axis)\n",
    "plt.plot(max_depth_range, RMSE_scores)\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('RMSE (lower is better)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=3, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_depth=3 was best, so fit a tree using that parameter\n",
    "treereg = DecisionTreeRegressor(max_depth=3, random_state=1)\n",
    "treereg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>year</td>\n",
       "      <td>0.798744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>miles</td>\n",
       "      <td>0.201256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doors</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vtype</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature  importance\n",
       "0    year    0.798744\n",
       "1   miles    0.201256\n",
       "2   doors    0.000000\n",
       "3   vtype    0.000000"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Gini importance\" of each feature: the (normalized) total reduction of error brought by that feature\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treereg.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tree diagram\n",
    "# create a Graphviz file\n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(treereg, out_file='tree_vehicles.dot', feature_names=feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the command line, run this to convert to PNG:\n",
    "#   dot -Tpng tree_vehicles.dot -o tree_vehicles.png\n",
    "\n",
    "\n",
    "# ![Tree for vehicle data](images/tree_vehicles.png)\n",
    "\n",
    "# Reading the internal nodes:\n",
    "# \n",
    "# - **samples:** number of observations in that node before splitting\n",
    "# - **mse:** MSE calculated by comparing the actual response values in that node against the mean response value in that node\n",
    "# - **rule:** rule used to split that node (go left if true, go right if false)\n",
    "# \n",
    "# Reading the leaves:\n",
    "# \n",
    "# - **samples:** number of observations in that node\n",
    "# - **value:** mean response value in that node\n",
    "# - **mse:** MSE calculated by comparing the actual response values in that node against \"value\"\n",
    "\n",
    "# ## Making predictions for the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>vtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>130000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6000</td>\n",
       "      <td>2005</td>\n",
       "      <td>82500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12000</td>\n",
       "      <td>2010</td>\n",
       "      <td>60000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  year   miles  doors  vtype\n",
       "0   3000  2003  130000      4      1\n",
       "1   6000  2005   82500      4      0\n",
       "2  12000  2010   60000      2      0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the testing data\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/vehicles_test.csv'\n",
    "test = pd.read_csv(url)\n",
    "test['vtype'] = test.vtype.map({'car':0, 'truck':1})\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4000.,  5000., 13500.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# **Question:** Using the tree diagram above, what predictions will the model make for each observation?\n",
    "\n",
    "# use fitted model to make predictions on testing data\n",
    "X_test = test[feature_cols]\n",
    "y_test = test.price\n",
    "y_pred = treereg.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1190.2380714238084"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate RMSE\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7937.253933193771"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate RMSE for your own tree!\n",
    "y_test = [3000, 6000, 12000]\n",
    "y_pred = [0, 0, 0]\n",
    "from sklearn import metrics\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Part 2: Classification trees\n",
    "# \n",
    "# **Example:** Predict whether Barack Obama or Hillary Clinton will win the Democratic primary in a particular county in 2008:\n",
    "\n",
    "# ![Obama-Clinton decision tree](images/obama_clinton_tree.jpg)\n",
    "\n",
    "# **Questions:**\n",
    "# \n",
    "# - What are the observations? How many observations are there?\n",
    "# - What is the response variable?\n",
    "# - What are the features?\n",
    "# - What is the most predictive feature?\n",
    "# - Why does the tree split on high school graduation rate twice in a row?\n",
    "# - What is the class prediction for the following county: 15% African-American, 90% high school graduation rate, located in the South, high poverty, high population density?\n",
    "# - What is the predicted probability for that same county?\n",
    "\n",
    "# ## Comparing regression trees and classification trees\n",
    "# \n",
    "# |regression trees|classification trees|\n",
    "# |---|---|\n",
    "# |predict a continuous response|predict a categorical response|\n",
    "# |predict using mean response of each leaf|predict using most commonly occuring class of each leaf|\n",
    "# |splits are chosen to minimize MSE|splits are chosen to minimize Gini index (discussed below)|\n",
    "\n",
    "# ## Splitting criteria for classification trees\n",
    "# \n",
    "# Common options for the splitting criteria:\n",
    "# \n",
    "# - **classification error rate:** fraction of training observations in a region that don't belong to the most common class\n",
    "# - **Gini index:** measure of total variance across classes in a region\n",
    "\n",
    "# ### Example of classification error rate\n",
    "# \n",
    "# Pretend we are predicting whether someone buys an iPhone or an Android:\n",
    "# \n",
    "# - At a particular node, there are **25 observations** (phone buyers), of whom **10 bought iPhones and 15 bought Androids**.\n",
    "# - Since the majority class is **Android**, that's our prediction for all 25 observations, and thus the classification error rate is **10/25 = 40%**.\n",
    "# \n",
    "# Our goal in making splits is to **reduce the classification error rate**. Let's try splitting on gender:\n",
    "# \n",
    "# - **Males:** 2 iPhones and 12 Androids, thus the predicted class is Android\n",
    "# - **Females:** 8 iPhones and 3 Androids, thus the predicted class is iPhone\n",
    "# - Classification error rate after this split would be **5/25 = 20%**\n",
    "# \n",
    "# Compare that with a split on age:\n",
    "# \n",
    "# - **30 or younger:** 4 iPhones and 8 Androids, thus the predicted class is Android\n",
    "# - **31 or older:** 6 iPhones and 7 Androids, thus the predicted class is Android\n",
    "# - Classification error rate after this split would be **10/25 = 40%**\n",
    "# \n",
    "# The decision tree algorithm will try **every possible split across all features**, and choose the split that **reduces the error rate the most.**\n",
    "\n",
    "# ### Example of Gini index\n",
    "# \n",
    "# Calculate the Gini index before making a split:\n",
    "# \n",
    "# $$1 - \\left(\\frac {iPhone} {Total}\\right)^2 - \\left(\\frac {Android} {Total}\\right)^2 = 1 - \\left(\\frac {10} {25}\\right)^2 - \\left(\\frac {15} {25}\\right)^2 = 0.48$$\n",
    "# \n",
    "# - The **maximum value** of the Gini index is 0.5, and occurs when the classes are perfectly balanced in a node.\n",
    "# - The **minimum value** of the Gini index is 0, and occurs when there is only one class represented in a node.\n",
    "# - A node with a lower Gini index is said to be more \"pure\".\n",
    "# \n",
    "# Evaluating the split on **gender** using Gini index:\n",
    "# \n",
    "# $$\\text{Males: } 1 - \\left(\\frac {2} {14}\\right)^2 - \\left(\\frac {12} {14}\\right)^2 = 0.24$$\n",
    "# $$\\text{Females: } 1 - \\left(\\frac {8} {11}\\right)^2 - \\left(\\frac {3} {11}\\right)^2 = 0.40$$\n",
    "# $$\\text{Weighted Average: } 0.24 \\left(\\frac {14} {25}\\right) + 0.40 \\left(\\frac {11} {25}\\right) = 0.31$$\n",
    "# \n",
    "# Evaluating the split on **age** using Gini index:\n",
    "# \n",
    "# $$\\text{30 or younger: } 1 - \\left(\\frac {4} {12}\\right)^2 - \\left(\\frac {8} {12}\\right)^2 = 0.44$$\n",
    "# $$\\text{31 or older: } 1 - \\left(\\frac {6} {13}\\right)^2 - \\left(\\frac {7} {13}\\right)^2 = 0.50$$\n",
    "# $$\\text{Weighted Average: } 0.44 \\left(\\frac {12} {25}\\right) + 0.50 \\left(\\frac {13} {25}\\right) = 0.47$$\n",
    "# \n",
    "# Again, the decision tree algorithm will try **every possible split**, and will choose the split that **reduces the Gini index (and thus increases the \"node purity\") the most.**\n",
    "\n",
    "# ### Comparing classification error rate and Gini index\n",
    "# \n",
    "# - Gini index is generally preferred because it will make splits that **increase node purity**, even if that split does not change the classification error rate.\n",
    "# - Node purity is important because we're interested in the **class proportions** in each region, since that's how we calculate the **predicted probability** of each class.\n",
    "# - scikit-learn's default splitting criteria for classification trees is Gini index.\n",
    "# \n",
    "# Note: There is another common splitting criteria called **cross-entropy**. It's numerically similar to Gini index, but slower to compute, thus it's not as popular as Gini index.\n",
    "\n",
    "# ## Building a classification tree in scikit-learn\n",
    "\n",
    "# We'll build a classification tree using the Titanic data:\n",
    "\n",
    "# read in the data\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/titanic.csv'\n",
    "titanic = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode female as 0 and male as 1\n",
    "titanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the missing values for age with the median age\n",
    "titanic.Age.fillna(titanic.Age.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame of dummy variables for Embarked\n",
    "embarked_dummies = pd.get_dummies(titanic.Embarked, prefix='Embarked')\n",
    "embarked_dummies.drop(embarked_dummies.columns[0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the original DataFrame and the dummy DataFrame\n",
    "titanic = pd.concat([titanic, embarked_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name  Sex   Age  SibSp  Parch  \\\n",
       "0                            Braund, Mr. Owen Harris    1  22.0      1      0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    0  38.0      1      0   \n",
       "2                             Heikkinen, Miss. Laina    0  26.0      0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    0  35.0      1      0   \n",
       "4                           Allen, Mr. William Henry    1  35.0      0      0   \n",
       "\n",
       "             Ticket     Fare Cabin Embarked  Embarked_Q  Embarked_S  \n",
       "0         A/5 21171   7.2500   NaN        S           0           1  \n",
       "1          PC 17599  71.2833   C85        C           0           0  \n",
       "2  STON/O2. 3101282   7.9250   NaN        S           0           1  \n",
       "3            113803  53.1000  C123        S           0           1  \n",
       "4            373450   8.0500   NaN        S           0           1  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the updated DataFrame\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - **Survived:** 0=died, 1=survived (response variable)\n",
    "# - **Pclass:** 1=first class, 2=second class, 3=third class\n",
    "#     - What will happen if the tree splits on this feature?\n",
    "# - **Sex:** 0=female, 1=male\n",
    "# - **Age:** numeric value\n",
    "# - **Embarked:** C or Q or S\n",
    "\n",
    "# define X and y\n",
    "feature_cols = ['Pclass', 'Sex', 'Age', 'Embarked_Q', 'Embarked_S']\n",
    "X = titanic[feature_cols]\n",
    "y = titanic.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit a classification tree with max_depth=3 on all data\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "treeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
    "treeclf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pclass</td>\n",
       "      <td>0.238656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0.622432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Age</td>\n",
       "      <td>0.080200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Embarked_Q</td>\n",
       "      <td>0.010298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Embarked_S</td>\n",
       "      <td>0.048413</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  importance\n",
       "0      Pclass    0.238656\n",
       "1         Sex    0.622432\n",
       "2         Age    0.080200\n",
       "3  Embarked_Q    0.010298\n",
       "4  Embarked_S    0.048413"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# At the command line, run this to convert to PNG:\n",
    "#   dot -Tpng tree_titanic.dot -o tree_titanic.png\n",
    "\n",
    "\n",
    "# ![Tree for Titanic data](images/tree_titanic.png)\n",
    "\n",
    "# Notice the split in the bottom right: the **same class** is predicted in both of its leaves. That split didn't affect the **classification error rate**, though it did increase the **node purity**, which is important because it increases the accuracy of our predicted probabilities.\n",
    "\n",
    "# compute the feature importances\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treeclf.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Part 3: Comparing decision trees with other models\n",
    "# \n",
    "# **Advantages of decision trees:**\n",
    "# \n",
    "# - Can be used for regression or classification\n",
    "# - Can be displayed graphically\n",
    "# - Highly interpretable\n",
    "# - Can be specified as a series of rules, and more closely approximate human decision-making than other models\n",
    "# - Prediction is fast\n",
    "# - Features don't need scaling\n",
    "# - Automatically learns feature interactions\n",
    "# - Tends to ignore irrelevant features\n",
    "# - Non-parametric (will outperform linear models if relationship between features and response is highly non-linear)\n",
    "\n",
    "# ![Trees versus linear models](images/tree_vs_linear.png)\n",
    "\n",
    "# **Disadvantages of decision trees:**\n",
    "# \n",
    "# - Performance is (generally) not competitive with the best supervised learning methods\n",
    "# - Can easily overfit the training data (tuning is required)\n",
    "# - Small variations in the data can result in a completely different tree (high variance)\n",
    "# - Recursive binary splitting makes \"locally optimal\" decisions that may not result in a globally optimal tree\n",
    "# - Doesn't tend to work well if the classes are highly unbalanced\n",
    "# - Doesn't tend to work well with very small datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Set Evaluation \n",
    "def Evaluation(X_test,y_test):\n",
    "        print('Accuracy score ', metrics.accuracy_score(treeclf.predict(X_train),y_train))\n",
    "        print('Auc',metrics.roc_auc_score(treeclf.predict(X_test),y_test))\n",
    "        print('F1 Score',metrics.f1_score(treeclf.predict(X_test),y_test))\n",
    "        print('log loss',metrics.log_loss(treeclf.predict(X_test),y_test))\n",
    "        print('Confusion Metrics',pd.crosstab(treeclf.predict(X_test),y_test))\n",
    "        print('precssion',metrics.precision_score(treeclf.predict(X_test),y_test))\n",
    "        print('recall',metrics.recall_score(treeclf.predict(X_test),y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score  0.8305369127516778\n",
      "Auc 0.8149051490514906\n",
      "F1 Score 0.7238095238095237\n",
      "log loss 6.790793943022876\n",
      "Confusion Metrics col_0    0   1\n",
      "row_0         \n",
      "0      161  44\n",
      "1       14  76\n",
      "precssion 0.6333333333333333\n",
      "recall 0.8444444444444444\n"
     ]
    }
   ],
   "source": [
    "Evaluation(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score  0.8305369127516778\n",
      "Auc 0.8537019123446685\n",
      "F1 Score 0.7292225201072385\n",
      "log loss 5.853163055813718\n",
      "Confusion Metrics Survived    0    1\n",
      "row_0             \n",
      "0         359   86\n",
      "1          15  136\n",
      "precssion 0.6126126126126126\n",
      "recall 0.9006622516556292\n"
     ]
    }
   ],
   "source": [
    "Evaluation(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.79      0.85       205\n",
      "           1       0.63      0.84      0.72        90\n",
      "\n",
      "   micro avg       0.80      0.80      0.80       295\n",
      "   macro avg       0.78      0.81      0.79       295\n",
      "weighted avg       0.83      0.80      0.81       295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://www.geeksforgeeks.org/confusion-matrix-machine-learning/\n",
    "print(metrics.classification_report(treeclf.predict(X_test),y_test,))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
