{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Decision Trees\n",
    "# \n",
    "# *Adapted from Chapter 8 of [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)*\n",
    "\n",
    "# Why are we learning about decision trees?\n",
    "# \n",
    "# - Can be applied to both regression and classification problems\n",
    "# - Many useful properties\n",
    "# - Very popular\n",
    "# - Basis for more sophisticated models\n",
    "# - Have a different way of \"thinking\" than the other models we have studied\n",
    "\n",
    "# ## Lesson objectives\n",
    "# \n",
    "# Students will be able to:\n",
    "# \n",
    "# - Explain how a decision tree is created\n",
    "# - Build a decision tree model in scikit-learn\n",
    "# - Tune a decision tree model and explain how tuning impacts the model\n",
    "# - Interpret a tree diagram\n",
    "# - Describe the key differences between regression and classification trees\n",
    "# - Decide whether a decision tree is an appropriate model for a given problem\n",
    "\n",
    "# # Part 1: Regression trees\n",
    "# \n",
    "# Major League Baseball player data from 1986-87:\n",
    "# \n",
    "# - **Years** (x-axis): number of years playing in the major leagues\n",
    "# - **Hits** (y-axis): number of hits in the previous year\n",
    "# - **Salary** (color): low salary is blue/green, high salary is red/yellow\n",
    "\n",
    "# ![Salary data](images/salary_color.png)\n",
    "\n",
    "# Group exercise:\n",
    "# \n",
    "# - The data above is our **training data**.\n",
    "# - We want to build a model that predicts the Salary of **future players** based on Years and Hits.\n",
    "# - We are going to \"segment\" the feature space into regions, and then use the **mean Salary in each region** as the predicted Salary for future players.\n",
    "# - Intuitively, you want to **maximize** the similarity (or \"homogeneity\") within a given region, and **minimize** the similarity between different regions.\n",
    "# \n",
    "# Rules for segmenting:\n",
    "# \n",
    "# - You can only use **straight lines**, drawn one at a time.\n",
    "# - Your line must either be **vertical or horizontal**.\n",
    "# - Your line **stops** when it hits an existing line.\n",
    "\n",
    "# ![Salary regions](images/salary_regions.png)\n",
    "\n",
    "# Above are the regions created by a computer:\n",
    "# \n",
    "# - $R_1$: players with **less than 5 years** of experience, mean Salary of **\\$166,000 **\n",
    "# - $R_2$: players with **5 or more years** of experience and **less than 118 hits**, mean Salary of **\\$403,000 **\n",
    "# - $R_3$: players with **5 or more years** of experience and **118 hits or more**, mean Salary of **\\$846,000 **\n",
    "# \n",
    "# **Note:** Years and Hits are both integers, but the convention is to use the **midpoint** between adjacent values to label a split.\n",
    "# \n",
    "# These regions are used to make predictions on **out-of-sample data**. Thus, there are only three possible predictions! (Is this different from how **linear regression** makes predictions?)\n",
    "# \n",
    "# Below is the equivalent regression tree:\n",
    "\n",
    "# ![Salary tree](images/salary_tree.png)\n",
    "\n",
    "# The first split is **Years < 4.5**, thus that split goes at the top of the tree. When a splitting rule is **True**, you follow the left branch. When a splitting rule is **False**, you follow the right branch.\n",
    "# \n",
    "# For players in the **left branch**, the mean Salary is \\$166,000, thus you label it with that value. (Salary has been divided by 1000 and log-transformed to 5.11.)\n",
    "# \n",
    "# For players in the **right branch**, there is a further split on **Hits < 117.5**, dividing players into two more Salary regions: \\$403,000 (transformed to 6.00), and \\$846,000 (transformed to 6.74).\n",
    "\n",
    "# ![Salary tree annotated](images/salary_tree_annotated.png)\n",
    "\n",
    "# **What does this tree tell you about your data?**\n",
    "# \n",
    "# - Years is the most important factor determining Salary, with a lower number of Years corresponding to a lower Salary.\n",
    "# - For a player with a lower number of Years, Hits is not an important factor determining Salary.\n",
    "# - For a player with a higher number of Years, Hits is an important factor determining Salary, with a greater number of Hits corresponding to a higher Salary.\n",
    "# \n",
    "# **Question:** What do you like and dislike about decision trees so far?\n",
    "\n",
    "# ## Building a regression tree by hand\n",
    "# \n",
    "# Your **training data** is a tiny dataset of [used vehicle sale prices](https://raw.githubusercontent.com/justmarkham/DAT8/master/data/vehicles_train.csv). Your goal is to **predict price** for testing data.\n",
    "# \n",
    "# 1. Read the data into a Pandas DataFrame.\n",
    "# 2. Explore the data by sorting, plotting, or split-apply-combine (aka `group_by`).\n",
    "# 3. Decide which feature is the most important predictor, and use that to create your first splitting rule.\n",
    "#     - Only binary splits are allowed.\n",
    "# 4. After making your first split, split your DataFrame into two parts, and then explore each part to figure out what other splits to make.\n",
    "# 5. Stop making splits once you are convinced that it strikes a good balance between underfitting and overfitting.\n",
    "#     - Your goal is to build a model that generalizes well.\n",
    "#     - You are allowed to split on the same variable multiple times!\n",
    "# 6. Draw your tree, labeling the leaves with the mean price for the observations in that region.\n",
    "#     - Make sure nothing is backwards: You follow the **left branch** if the rule is true, and the **right branch** if the rule is false.\n",
    "\n",
    "# ## How does a computer build a regression tree?\n",
    "# \n",
    "# **Ideal approach:** Consider every possible partition of the feature space (computationally infeasible)\n",
    "# \n",
    "# **\"Good enough\" approach:** recursive binary splitting\n",
    "# \n",
    "# 1. Begin at the top of the tree.\n",
    "# 2. For **every feature**, examine **every possible cutpoint**, and choose the feature and cutpoint such that the resulting tree has the lowest possible mean squared error (MSE). Make that split.\n",
    "# 3. Examine the two resulting regions, and again make a **single split** (in one of the regions) to minimize the MSE.\n",
    "# 4. Keep repeating step 3 until a **stopping criterion** is met:\n",
    "#     - maximum tree depth (maximum number of splits required to arrive at a leaf)\n",
    "#     - minimum number of observations in a leaf\n",
    "\n",
    "# ### Demo: Choosing the ideal cutpoint for a given feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vehicle data\n",
    "import pandas as pd\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/vehicles_train.csv'\n",
    "train = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>vtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22000</td>\n",
       "      <td>2012</td>\n",
       "      <td>13000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13000</td>\n",
       "      <td>2010</td>\n",
       "      <td>73500</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9500</td>\n",
       "      <td>2009</td>\n",
       "      <td>78000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9000</td>\n",
       "      <td>2007</td>\n",
       "      <td>47000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  year  miles  doors vtype\n",
       "0  22000  2012  13000      2   car\n",
       "1  14000  2010  30000      2   car\n",
       "2  13000  2010  73500      4   car\n",
       "3   9500  2009  78000      4   car\n",
       "4   9000  2007  47000      4   car"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>vtype</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22000</td>\n",
       "      <td>2012</td>\n",
       "      <td>13000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13000</td>\n",
       "      <td>2010</td>\n",
       "      <td>73500</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9500</td>\n",
       "      <td>2009</td>\n",
       "      <td>78000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9000</td>\n",
       "      <td>2007</td>\n",
       "      <td>47000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4000</td>\n",
       "      <td>2006</td>\n",
       "      <td>124000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3000</td>\n",
       "      <td>2004</td>\n",
       "      <td>177000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000</td>\n",
       "      <td>2004</td>\n",
       "      <td>209000</td>\n",
       "      <td>4</td>\n",
       "      <td>truck</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>138000</td>\n",
       "      <td>2</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1900</td>\n",
       "      <td>2003</td>\n",
       "      <td>160000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2500</td>\n",
       "      <td>2003</td>\n",
       "      <td>190000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5000</td>\n",
       "      <td>2001</td>\n",
       "      <td>62000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1800</td>\n",
       "      <td>1999</td>\n",
       "      <td>163000</td>\n",
       "      <td>2</td>\n",
       "      <td>truck</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1300</td>\n",
       "      <td>1997</td>\n",
       "      <td>138000</td>\n",
       "      <td>4</td>\n",
       "      <td>car</td>\n",
       "      <td>6571.428571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  year   miles  doors  vtype   prediction\n",
       "0   22000  2012   13000      2    car  6571.428571\n",
       "1   14000  2010   30000      2    car  6571.428571\n",
       "2   13000  2010   73500      4    car  6571.428571\n",
       "3    9500  2009   78000      4    car  6571.428571\n",
       "4    9000  2007   47000      4    car  6571.428571\n",
       "5    4000  2006  124000      2    car  6571.428571\n",
       "6    3000  2004  177000      4    car  6571.428571\n",
       "7    2000  2004  209000      4  truck  6571.428571\n",
       "8    3000  2003  138000      2    car  6571.428571\n",
       "9    1900  2003  160000      4    car  6571.428571\n",
       "10   2500  2003  190000      2  truck  6571.428571\n",
       "11   5000  2001   62000      4    car  6571.428571\n",
       "12   1800  1999  163000      2  truck  6571.428571\n",
       "13   1300  1997  138000      4    car  6571.428571"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before splitting anything, just predict the mean of the entire dataset\n",
    "train['prediction'] = train.price.mean()\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5936.981985995983"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate RMSE for those predictions\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "np.sqrt(metrics.mean_squared_error(train.price, train.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that calculates the RMSE for a given split of miles\n",
    "def mileage_split(miles):\n",
    "    lower_mileage_price = train[train.miles < miles].price.mean()\n",
    "    higher_mileage_price = train[train.miles >= miles].price.mean()\n",
    "    train['prediction'] = np.where(train.miles < miles, lower_mileage_price, higher_mileage_price)\n",
    "    return np.sqrt(metrics.mean_squared_error(train.price, train.prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3984.0917425414564\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>vtype</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22000</td>\n",
       "      <td>2012</td>\n",
       "      <td>13000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>15000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14000</td>\n",
       "      <td>2010</td>\n",
       "      <td>30000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>15000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13000</td>\n",
       "      <td>2010</td>\n",
       "      <td>73500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9500</td>\n",
       "      <td>2009</td>\n",
       "      <td>78000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9000</td>\n",
       "      <td>2007</td>\n",
       "      <td>47000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>15000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4000</td>\n",
       "      <td>2006</td>\n",
       "      <td>124000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3000</td>\n",
       "      <td>2004</td>\n",
       "      <td>177000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000</td>\n",
       "      <td>2004</td>\n",
       "      <td>209000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>138000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1900</td>\n",
       "      <td>2003</td>\n",
       "      <td>160000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2500</td>\n",
       "      <td>2003</td>\n",
       "      <td>190000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>5000</td>\n",
       "      <td>2001</td>\n",
       "      <td>62000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1800</td>\n",
       "      <td>1999</td>\n",
       "      <td>163000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1300</td>\n",
       "      <td>1997</td>\n",
       "      <td>138000</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4272.727273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    price  year   miles  doors  vtype    prediction\n",
       "0   22000  2012   13000      2      0  15000.000000\n",
       "1   14000  2010   30000      2      0  15000.000000\n",
       "2   13000  2010   73500      4      0   4272.727273\n",
       "3    9500  2009   78000      4      0   4272.727273\n",
       "4    9000  2007   47000      4      0  15000.000000\n",
       "5    4000  2006  124000      2      0   4272.727273\n",
       "6    3000  2004  177000      4      0   4272.727273\n",
       "7    2000  2004  209000      4      1   4272.727273\n",
       "8    3000  2003  138000      2      0   4272.727273\n",
       "9    1900  2003  160000      4      0   4272.727273\n",
       "10   2500  2003  190000      2      1   4272.727273\n",
       "11   5000  2001   62000      4      0   4272.727273\n",
       "12   1800  1999  163000      2      1   4272.727273\n",
       "13   1300  1997  138000      4      0   4272.727273"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate RMSE for tree which splits on miles < 50000\n",
    "print ('RMSE:', mileage_split(50000))\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 3530.146530076269\n"
     ]
    }
   ],
   "source": [
    "# calculate RMSE for tree which splits on miles < 100000\n",
    "print ('RMSE:', mileage_split(100000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all possible mileage splits\n",
    "mileage_range = range(train.miles.min(), train.miles.max(), 1000)\n",
    "RMSE = [mileage_split(miles) for miles in mileage_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allow plots to appear in the notebook\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (6, 4)\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'RMSE (lower is better)')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot mileage cutpoint (x-axis) versus RMSE (y-axis)\n",
    "plt.plot(mileage_range, RMSE)\n",
    "plt.xlabel('Mileage cutpoint')\n",
    "plt.ylabel('RMSE (lower is better)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Recap:** Before every split, this process is repeated for every feature, and the feature and cutpoint that produces the lowest MSE is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Building a regression tree in scikit-learn\n",
    "# encode car as 0 and truck as 1\n",
    "train['vtype'] = train.vtype.map({'car':0, 'truck':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define X and y\n",
    "feature_cols = ['year', 'miles', 'doors', 'vtype']\n",
    "X = train[feature_cols]\n",
    "y = train.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate a DecisionTreeRegressor (with random_state=1)\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "treereg = DecisionTreeRegressor(random_state=1)\n",
    "treereg.fit(X,y)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use leave-one-out cross-validation (LOOCV) to estimate the RMSE for this model\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics as mt\n",
    "predict=treereg.predict(X)\n",
    "mt.mean_squared_error(y,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22000., 14000., 13000.,  9500.,  9000.,  4000.,  3000.,  2000.,\n",
       "        3000.,  1900.,  2500.,  5000.,  1800.,  1300.])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(treereg, X, y, cv=14)\n",
    "np.mean(np.sqrt(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## What happens when we grow a tree too deep?\n",
    "# \n",
    "# - Left: Regression tree for Salary **grown deeper**\n",
    "# - Right: Comparison of the **training, testing, and cross-validation errors** for trees with different numbers of leaves\n",
    "\n",
    "# ![Salary tree grown deep](images/salary_tree_deep.png)\n",
    "\n",
    "# The **training error** continues to go down as the tree size increases (due to overfitting), but the lowest **cross-validation error** occurs for a tree with 3 leaves.\n",
    "\n",
    "# ## Tuning a regression tree\n",
    "# \n",
    "# Let's try to reduce the RMSE by tuning the **max_depth** parameter:\n",
    "\n",
    "# try different values one-by-one\n",
    "treereg = DecisionTreeRegressor(max_depth=1, random_state=1)\n",
    "scores = cross_val_score(treereg, X, y, cv=14)\n",
    "np.mean(np.sqrt(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:13: RuntimeWarning: invalid value encountered in sqrt\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'RMSE (lower is better)')"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Or, we could write a loop to try a range of values:\n",
    "\n",
    "# list of values to try\n",
    "max_depth_range = range(1, 8)\n",
    "\n",
    "# list to store the average RMSE for each value of max_depth\n",
    "RMSE_scores = []\n",
    "\n",
    "# use LOOCV with each value of max_depth\n",
    "for depth in max_depth_range:\n",
    "    treereg = DecisionTreeRegressor(max_depth=depth, random_state=1)\n",
    "    MSE_scores = cross_val_score(treereg, X, y, cv=14)\n",
    "    RMSE_scores.append(np.mean(np.sqrt(-MSE_scores)))\n",
    "\n",
    "\n",
    "# plot max_depth (x-axis) versus RMSE (y-axis)\n",
    "plt.plot(max_depth_range, RMSE_scores)\n",
    "plt.xlabel('max_depth')\n",
    "plt.ylabel('RMSE (lower is better)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=3, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=1, splitter='best')"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max_depth=3 was best, so fit a tree using that parameter\n",
    "treereg = DecisionTreeRegressor(max_depth=3, random_state=1)\n",
    "treereg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>year</td>\n",
       "      <td>0.798744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>miles</td>\n",
       "      <td>0.201256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doors</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vtype</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature  importance\n",
       "0    year    0.798744\n",
       "1   miles    0.201256\n",
       "2   doors    0.000000\n",
       "3   vtype    0.000000"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"Gini importance\" of each feature: the (normalized) total reduction of error brought by that feature\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treereg.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Creating a tree diagram\n",
    "\n",
    "# create a Graphviz file\n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(treereg, out_file='tree_vehicles.dot', feature_names=feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the command line, run this to convert to PNG:\n",
    "#   dot -Tpng tree_vehicles.dot -o tree_vehicles.png\n",
    "\n",
    "\n",
    "# ![Tree for vehicle data](images/tree_vehicles.png)\n",
    "\n",
    "# Reading the internal nodes:\n",
    "# \n",
    "# - **samples:** number of observations in that node before splitting\n",
    "# - **mse:** MSE calculated by comparing the actual response values in that node against the mean response value in that node\n",
    "# - **rule:** rule used to split that node (go left if true, go right if false)\n",
    "# \n",
    "# Reading the leaves:\n",
    "# \n",
    "# - **samples:** number of observations in that node\n",
    "# - **value:** mean response value in that node\n",
    "# - **mse:** MSE calculated by comparing the actual response values in that node against \"value\"\n",
    "\n",
    "# ## Making predictions for the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>year</th>\n",
       "      <th>miles</th>\n",
       "      <th>doors</th>\n",
       "      <th>vtype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000</td>\n",
       "      <td>2003</td>\n",
       "      <td>130000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6000</td>\n",
       "      <td>2005</td>\n",
       "      <td>82500</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12000</td>\n",
       "      <td>2010</td>\n",
       "      <td>60000</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  year   miles  doors  vtype\n",
       "0   3000  2003  130000      4      1\n",
       "1   6000  2005   82500      4      0\n",
       "2  12000  2010   60000      2      0"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the testing data\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/vehicles_test.csv'\n",
    "test = pd.read_csv(url)\n",
    "test['vtype'] = test.vtype.map({'car':0, 'truck':1})\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4000.,  5000., 13500.])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# **Question:** Using the tree diagram above, what predictions will the model make for each observation?\n",
    "\n",
    "# use fitted model to make predictions on testing data\n",
    "X_test = test[feature_cols]\n",
    "y_test = test.price\n",
    "y_pred = treereg.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7937.253933193771"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate RMSE\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7937.253933193771"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate RMSE for your own tree!\n",
    "y_test = [3000, 6000, 12000]\n",
    "y_pred = [0, 0, 0]\n",
    "from sklearn import metrics\n",
    "np.sqrt(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Part 2: Classification trees\n",
    "# \n",
    "# **Example:** Predict whether Barack Obama or Hillary Clinton will win the Democratic primary in a particular county in 2008:\n",
    "\n",
    "# ![Obama-Clinton decision tree](images/obama_clinton_tree.jpg)\n",
    "\n",
    "# **Questions:**\n",
    "# \n",
    "# - What are the observations? How many observations are there?\n",
    "# - What is the response variable?\n",
    "# - What are the features?\n",
    "# - What is the most predictive feature?\n",
    "# - Why does the tree split on high school graduation rate twice in a row?\n",
    "# - What is the class prediction for the following county: 15% African-American, 90% high school graduation rate, located in the South, high poverty, high population density?\n",
    "# - What is the predicted probability for that same county?\n",
    "\n",
    "# ## Comparing regression trees and classification trees\n",
    "# \n",
    "# |regression trees|classification trees|\n",
    "# |---|---|\n",
    "# |predict a continuous response|predict a categorical response|\n",
    "# |predict using mean response of each leaf|predict using most commonly occuring class of each leaf|\n",
    "# |splits are chosen to minimize MSE|splits are chosen to minimize Gini index (discussed below)|\n",
    "\n",
    "# ## Splitting criteria for classification trees\n",
    "# \n",
    "# Common options for the splitting criteria:\n",
    "# \n",
    "# - **classification error rate:** fraction of training observations in a region that don't belong to the most common class\n",
    "# - **Gini index:** measure of total variance across classes in a region\n",
    "\n",
    "# ### Example of classification error rate\n",
    "# \n",
    "# Pretend we are predicting whether someone buys an iPhone or an Android:\n",
    "# \n",
    "# - At a particular node, there are **25 observations** (phone buyers), of whom **10 bought iPhones and 15 bought Androids**.\n",
    "# - Since the majority class is **Android**, that's our prediction for all 25 observations, and thus the classification error rate is **10/25 = 40%**.\n",
    "# \n",
    "# Our goal in making splits is to **reduce the classification error rate**. Let's try splitting on gender:\n",
    "# \n",
    "# - **Males:** 2 iPhones and 12 Androids, thus the predicted class is Android\n",
    "# - **Females:** 8 iPhones and 3 Androids, thus the predicted class is iPhone\n",
    "# - Classification error rate after this split would be **5/25 = 20%**\n",
    "# \n",
    "# Compare that with a split on age:\n",
    "# \n",
    "# - **30 or younger:** 4 iPhones and 8 Androids, thus the predicted class is Android\n",
    "# - **31 or older:** 6 iPhones and 7 Androids, thus the predicted class is Android\n",
    "# - Classification error rate after this split would be **10/25 = 40%**\n",
    "# \n",
    "# The decision tree algorithm will try **every possible split across all features**, and choose the split that **reduces the error rate the most.**\n",
    "\n",
    "# ### Example of Gini index\n",
    "# \n",
    "# Calculate the Gini index before making a split:\n",
    "# \n",
    "# $$1 - \\left(\\frac {iPhone} {Total}\\right)^2 - \\left(\\frac {Android} {Total}\\right)^2 = 1 - \\left(\\frac {10} {25}\\right)^2 - \\left(\\frac {15} {25}\\right)^2 = 0.48$$\n",
    "# \n",
    "# - The **maximum value** of the Gini index is 0.5, and occurs when the classes are perfectly balanced in a node.\n",
    "# - The **minimum value** of the Gini index is 0, and occurs when there is only one class represented in a node.\n",
    "# - A node with a lower Gini index is said to be more \"pure\".\n",
    "# \n",
    "# Evaluating the split on **gender** using Gini index:\n",
    "# \n",
    "# $$\\text{Males: } 1 - \\left(\\frac {2} {14}\\right)^2 - \\left(\\frac {12} {14}\\right)^2 = 0.24$$\n",
    "# $$\\text{Females: } 1 - \\left(\\frac {8} {11}\\right)^2 - \\left(\\frac {3} {11}\\right)^2 = 0.40$$\n",
    "# $$\\text{Weighted Average: } 0.24 \\left(\\frac {14} {25}\\right) + 0.40 \\left(\\frac {11} {25}\\right) = 0.31$$\n",
    "# \n",
    "# Evaluating the split on **age** using Gini index:\n",
    "# \n",
    "# $$\\text{30 or younger: } 1 - \\left(\\frac {4} {12}\\right)^2 - \\left(\\frac {8} {12}\\right)^2 = 0.44$$\n",
    "# $$\\text{31 or older: } 1 - \\left(\\frac {6} {13}\\right)^2 - \\left(\\frac {7} {13}\\right)^2 = 0.50$$\n",
    "# $$\\text{Weighted Average: } 0.44 \\left(\\frac {12} {25}\\right) + 0.50 \\left(\\frac {13} {25}\\right) = 0.47$$\n",
    "# \n",
    "# Again, the decision tree algorithm will try **every possible split**, and will choose the split that **reduces the Gini index (and thus increases the \"node purity\") the most.**\n",
    "\n",
    "# ### Comparing classification error rate and Gini index\n",
    "# \n",
    "# - Gini index is generally preferred because it will make splits that **increase node purity**, even if that split does not change the classification error rate.\n",
    "# - Node purity is important because we're interested in the **class proportions** in each region, since that's how we calculate the **predicted probability** of each class.\n",
    "# - scikit-learn's default splitting criteria for classification trees is Gini index.\n",
    "# \n",
    "# Note: There is another common splitting criteria called **cross-entropy**. It's numerically similar to Gini index, but slower to compute, thus it's not as popular as Gini index.\n",
    "\n",
    "# ## Building a classification tree in scikit-learn\n",
    "\n",
    "# We'll build a classification tree using the Titanic data:\n",
    "\n",
    "# read in the data\n",
    "url = 'https://raw.githubusercontent.com/justmarkham/DAT8/master/data/titanic.csv'\n",
    "titanic = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode female as 0 and male as 1\n",
    "titanic['Sex'] = titanic.Sex.map({'female':0, 'male':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill in the missing values for age with the median age\n",
    "titanic.Age.fillna(titanic.Age.median(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a DataFrame of dummy variables for Embarked\n",
    "embarked_dummies = pd.get_dummies(titanic.Embarked, prefix='Embarked')\n",
    "embarked_dummies.drop(embarked_dummies.columns[0], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the original DataFrame and the dummy DataFrame\n",
    "titanic = pd.concat([titanic, embarked_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Embarked_Q</th>\n",
       "      <th>Embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name  Sex   Age  SibSp  Parch  \\\n",
       "0                            Braund, Mr. Owen Harris    1  22.0      1      0   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...    0  38.0      1      0   \n",
       "2                             Heikkinen, Miss. Laina    0  26.0      0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    0  35.0      1      0   \n",
       "4                           Allen, Mr. William Henry    1  35.0      0      0   \n",
       "\n",
       "             Ticket     Fare Cabin Embarked  Embarked_Q  Embarked_S  \n",
       "0         A/5 21171   7.2500   NaN        S           0           1  \n",
       "1          PC 17599  71.2833   C85        C           0           0  \n",
       "2  STON/O2. 3101282   7.9250   NaN        S           0           1  \n",
       "3            113803  53.1000  C123        S           0           1  \n",
       "4            373450   8.0500   NaN        S           0           1  "
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the updated DataFrame\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - **Survived:** 0=died, 1=survived (response variable)\n",
    "# - **Pclass:** 1=first class, 2=second class, 3=third class\n",
    "#     - What will happen if the tree splits on this feature?\n",
    "# - **Sex:** 0=female, 1=male\n",
    "# - **Age:** numeric value\n",
    "# - **Embarked:** C or Q or S\n",
    "\n",
    "# define X and y\n",
    "feature_cols = ['Pclass', 'Sex', 'Age', 'Embarked_Q', 'Embarked_S']\n",
    "X = titanic[feature_cols]\n",
    "y = titanic.Survived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=1,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit a classification tree with max_depth=3 on all data\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "treeclf = DecisionTreeClassifier(max_depth=3, random_state=1)\n",
    "treeclf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Graphviz file\n",
    "export_graphviz(treeclf, out_file='tree_titanic.dot', feature_names=feature_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pclass</td>\n",
       "      <td>0.242664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sex</td>\n",
       "      <td>0.655584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Age</td>\n",
       "      <td>0.064494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Embarked_Q</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Embarked_S</td>\n",
       "      <td>0.037258</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  importance\n",
       "0      Pclass    0.242664\n",
       "1         Sex    0.655584\n",
       "2         Age    0.064494\n",
       "3  Embarked_Q    0.000000\n",
       "4  Embarked_S    0.037258"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# At the command line, run this to convert to PNG:\n",
    "#   dot -Tpng tree_titanic.dot -o tree_titanic.png\n",
    "\n",
    "\n",
    "# ![Tree for Titanic data](images/tree_titanic.png)\n",
    "\n",
    "# Notice the split in the bottom right: the **same class** is predicted in both of its leaves. That split didn't affect the **classification error rate**, though it did increase the **node purity**, which is important because it increases the accuracy of our predicted probabilities.\n",
    "\n",
    "# compute the feature importances\n",
    "pd.DataFrame({'feature':feature_cols, 'importance':treeclf.feature_importances_})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Part 3: Comparing decision trees with other models\n",
    "# \n",
    "# **Advantages of decision trees:**\n",
    "# \n",
    "# - Can be used for regression or classification\n",
    "# - Can be displayed graphically\n",
    "# - Highly interpretable\n",
    "# - Can be specified as a series of rules, and more closely approximate human decision-making than other models\n",
    "# - Prediction is fast\n",
    "# - Features don't need scaling\n",
    "# - Automatically learns feature interactions\n",
    "# - Tends to ignore irrelevant features\n",
    "# - Non-parametric (will outperform linear models if relationship between features and response is highly non-linear)\n",
    "\n",
    "# ![Trees versus linear models](images/tree_vs_linear.png)\n",
    "\n",
    "# **Disadvantages of decision trees:**\n",
    "# \n",
    "# - Performance is (generally) not competitive with the best supervised learning methods\n",
    "# - Can easily overfit the training data (tuning is required)\n",
    "# - Small variations in the data can result in a completely different tree (high variance)\n",
    "# - Recursive binary splitting makes \"locally optimal\" decisions that may not result in a globally optimal tree\n",
    "# - Doesn't tend to work well if the classes are highly unbalanced\n",
    "# - Doesn't tend to work well with very small datasets"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
